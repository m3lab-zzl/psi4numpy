{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本章小结：\n",
    "\n",
    "- 项目对张量使用**广义**爱因斯坦求和约定\n",
    "- 代码层面，总和可读性与效率，最优的选择是numpy(>1.12)的 np.einsum('ij,jk,kl->il', A, B, C, optimize=True)函数\n",
    "- 最高效的是np.dot，因为调用的是BLAS库，但是所需的额外优化代码量很大，而且不是那么直观\n",
    "- 最低效的是不带optimize的np.einsum\n",
    "\n",
    "# Tensor Manipulation: Psi4 and NumPy manipulation routines\n",
    "\n",
    "爱因斯坦求和约定，用于计算内积：\n",
    "\n",
    "$$c = \\sum_{ij} A_{ij} * B_{ij}$$\n",
    "\n",
    "可以简写成：\n",
    "\n",
    "$$c = A_{ij} * B_{ij}$$\n",
    "\n",
    "另外，用于矩阵时：\n",
    "\n",
    "\\begin{align}\n",
    "\\rm{Conventional}\\;\\;\\;  C_{ik} &= \\sum_{j} A_{ij} * B_{jk} \\\\\n",
    "\\rm{Einstein}\\;\\;\\;  C &= A_{ij} * B_{jk} \\\\\n",
    "\\end{align}\n",
    "\n",
    "然而，有些情况下，这个求和约定无法使用，此时需要适当扩展记号，比如对于矩阵的星乘（元素相乘）：\n",
    "\n",
    "$$C_{ij} = \\sum_{ij} A_{ij} * B_{ij}$$\n",
    "\n",
    "左边就必须加上下标，否则无法区分是点乘还是星乘。\n",
    "\n",
    "$$C_{ij} = A_{ij} * B_{ij}$$\n",
    "\n",
    "对于矩阵：\n",
    "\n",
    "\\begin{align}\n",
    "{\\rm Matrix}\\;\\;\\;  \\bf{D} &= \\bf{A B C} \\\\\n",
    "{\\rm Einstein}\\;\\;\\;  D_{il} &= A_{ij} B_{jk} C_{kl}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.03932478 1.372394  ]\n",
      " [0.81406273 1.20047283]]\n",
      "[[0.7394282  0.21402241 0.08587418]\n",
      " [0.28354254 0.80258753 0.11434275]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = np.random.random((2,3))\n",
    "B = np.random.random((2,3))\n",
    "c = np.inner(A,B)\n",
    "print(c)\n",
    "\n",
    "C = A*B\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Einsum\n",
    "\n",
    "To perform most operations we turn to [NumPy's einsum function](https://docs.scipy.org/doc/numpy/reference/generated/numpy.einsum.html) which allows the Einsten convention as an input. In addition to being much easier to read, manipulate, and change, it is also much more efficient that a pure Python implementation.\n",
    "\n",
    "To begin let us consider the construction of the following tensor (which you may recognize):\n",
    "$$G_{pq} = 2.0 * I_{pqrs} D_{rs} - 1.0 * I_{prqs} D_{rs}$$ \n",
    "\n",
    "First let us import our normal suite of modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import psi4\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use conventional Python loops and einsum to perform the same task. Keep size relatively small as these 4-index tensors grow very quickly in size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loop and einsum fock builds match:    True\n",
      "\n",
      "Time for loop G build:           0.4252 seconds\n",
      "Time for einsum G build:         0.0277 seconds\n",
      "G builds with einsum are 15.3570 times faster than Python loops!\n"
     ]
    }
   ],
   "source": [
    "size = 20\n",
    "\n",
    "if size > 30:\n",
    "    raise Exception(\"Size must be smaller than 30.\")\n",
    "D = np.random.rand(size, size)\n",
    "I = np.random.rand(size, size, size, size)\n",
    "\n",
    "# Build the fock matrix using loops, while keeping track of time\n",
    "tstart_loop = time.time()\n",
    "Gloop = np.zeros((size, size))\n",
    "for p in range(size):\n",
    "    for q in range(size):\n",
    "        for r in range(size):\n",
    "            for s in range(size):\n",
    "                Gloop[p, q] += 2 * I[p, q, r, s] * D[r, s]\n",
    "                Gloop[p, q] -=     I[p, r, q, s] * D[r, s]\n",
    "\n",
    "g_loop_time = time.time() - tstart_loop\n",
    "\n",
    "# Build the fock matrix using einsum, while keeping track of time\n",
    "tstart_einsum = time.time()\n",
    "J = np.einsum('pqrs,rs', I, D, optimize=True)\n",
    "K = np.einsum('prqs,rs', I, D, optimize=True)\n",
    "G = 2 * J - K\n",
    "\n",
    "einsum_time = time.time() - tstart_einsum\n",
    "\n",
    "# Make sure the correct answer is obtained\n",
    "print('The loop and einsum fock builds match:    %s\\n' % np.allclose(G, Gloop))\n",
    "# Print out relative times for explicit loop vs einsum Fock builds\n",
    "print('Time for loop G build:   %14.4f seconds' % g_loop_time)\n",
    "print('Time for einsum G build: %14.4f seconds' % einsum_time)\n",
    "print('G builds with einsum are {:3.4f} times faster than Python loops!'.format(g_loop_time / einsum_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the einsum function is considerably faster than the pure Python loops and, in this author's opinion, much cleaner and easier to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dot\n",
    "\n",
    "Now let us turn our attention to a more canonical matrix multiplication example such as:\n",
    "$$D_{il} = A_{ij} B_{jk} C_{kl}$$\n",
    "\n",
    "We could perform this operation using einsum; however, matrix multiplication is an extremely common operation in all branches of linear algebra. Thus, these functions have been optimized to be more efficient than the `einsum` function. The matrix product will explicitly compute the following operation:\n",
    "$$C_{ij} = A_{ij} * B_{ij}$$\n",
    "\n",
    "This can be called with [NumPy's dot function](https://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html#numpy.dot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pair product allclose: True\n"
     ]
    }
   ],
   "source": [
    "size = 200\n",
    "A = np.random.rand(size, size)\n",
    "B = np.random.rand(size, size)\n",
    "C = np.random.rand(size, size)\n",
    "\n",
    "# First compute the pair product\n",
    "tmp_dot = np.dot(A, B)\n",
    "tmp_einsum = np.einsum('ij,jk->ik', A, B, optimize=True)\n",
    "print(\"Pair product allclose: %s\" % np.allclose(tmp_dot, tmp_einsum))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have proved exactly what the dot product does, let us consider the full chain and do a timing comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chain multiplication allclose: True\n",
      "\n",
      "np.dot time:\n",
      "1.25 ms ± 255 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "\n",
      "np.einsum time\n",
      "1.89 s ± 56.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "D_dot = np.dot(A, B).dot(C)\n",
    "D_einsum = np.einsum('ij,jk,kl->il', A, B, C, optimize=True)\n",
    "print(\"Chain multiplication allclose: %s\" % np.allclose(D_dot, D_einsum))\n",
    "\n",
    "print(\"\\nnp.dot time:\")\n",
    "%timeit np.dot(A, B).dot(C)\n",
    "\n",
    "print(\"\\nnp.einsum time\")\n",
    "# no optimization here for illustrative purposes!\n",
    "%timeit np.einsum('ij,jk,kl->il', A, B, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On most machines the `np.dot` times are roughly ~2,000 times faster. The reason is twofold:\n",
    " - The `np.dot` routines typically call [Basic Linear Algebra Subprograms (BLAS)](https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms). The BLAS routines are highly optimized and threaded versions of the code.\n",
    " - The `np.einsum` code will not factorize the operation by default; Thus, the overall cost is ${\\cal O}(N^4)$ (as there are four indices) rather than the factored $(\\bf{A B}) \\bf{C}$ which runs ${\\cal O}(N^3)$.\n",
    " \n",
    "The first issue is difficult to overcome; however, the second issue can be resolved by the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np.einsum factorized time:\n",
      "6.93 ms ± 294 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "print(\"np.einsum factorized time:\")\n",
    "# no optimization here for illustrative purposes!\n",
    "%timeit np.einsum('ik,kl->il', np.einsum('ij,jk->ik', A, B), C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On most machines the factorized `einsum` expression is only ~10 times slower than `np.dot`. While a massive improvement, this is a clear demonstration the BLAS usage is usually recommended. It is a tradeoff between speed and readability. The Psi4NumPy project tends to lean toward `einsum` usage except in case where the benefit is too large to pass up.\n",
    "\n",
    "Starting in NumPy 1.12, the [einsum function](https://docs.scipy.org/doc/numpy/reference/generated/numpy.einsum.html) has a `optimize` flag which will automatically factorize the einsum code for you using a greedy algorithm, leading to considerable speedups at almost no cost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "np.einsum optimized time\n",
      "1.57 ms ± 210 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nnp.einsum optimized time\")\n",
    "%timeit np.einsum('ij,jk,kl->il', A, B, C, optimize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, using `optimize=True` for automatic factorization is only 25% slower than `np.dot`. Furthermore, it is ~5 times faster than factorizing the expression by hand, which represents a very good trade-off between speed and readability. When unsure, `optimize=True` is strongly recommended."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complex tensor manipulations\n",
    "\n",
    "用一个更复杂的例子展示这些方法之间的效率差别：\n",
    "\n",
    "Let us consider a popular index transformation example:\n",
    "$$M_{pqrs} = C_{pi} C_{qj} I_{ijkl} C_{rk} C_{sl}$$\n",
    "\n",
    "Here, a naive `einsum` call would scale like $\\mathcal{O}(N^8)$ which translates to an extremely costly computation for all but the smallest $N$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Numpy's N^8 transformation...\n",
      "...transformation complete in 26.988 seconds.\n",
      "\n",
      "\n",
      "Starting Numpy's N^5 transformation with einsum...\n",
      "...transformation complete in 0.004 seconds.\n",
      "\n",
      "N^5 6144.96 faster than N^8 algorithm!\n",
      "Allclose: True\n",
      "\n",
      "Now Numpy's optimized transformation...\n",
      "...optimized transformation complete in 0.002 seconds.\n",
      "\n",
      "\n",
      "Starting Numpy's N^5 transformation with dot...\n",
      "...transformation complete in 0.001 seconds.\n",
      "\n",
      "Allclose: True\n",
      "N^5 18744.20 faster than N^8 algorithm!\n"
     ]
    }
   ],
   "source": [
    "# Grab orbitals\n",
    "size = 15\n",
    "if size > 15:\n",
    "    raise Exception(\"Size must be smaller than 15.\")\n",
    "    \n",
    "C = np.random.rand(size, size)\n",
    "I = np.random.rand(size, size, size, size)\n",
    "\n",
    "# Numpy einsum N^8 transformation.\n",
    "print(\"\\nStarting Numpy's N^8 transformation...\")\n",
    "n8_tstart = time.time()\n",
    "# no optimization here for illustrative purposes!\n",
    "MO_n8 = np.einsum('pI,qJ,pqrs,rK,sL->IJKL', C, C, I, C, C)\n",
    "n8_time = time.time() - n8_tstart\n",
    "print(\"...transformation complete in %.3f seconds.\" % (n8_time))\n",
    "\n",
    "# Numpy einsum N^5 transformation.\n",
    "print(\"\\n\\nStarting Numpy's N^5 transformation with einsum...\")\n",
    "n5_tstart = time.time()\n",
    "# no optimization here for illustrative purposes!\n",
    "MO_n5 = np.einsum('pA,pqrs->Aqrs', C, I)\n",
    "MO_n5 = np.einsum('qB,Aqrs->ABrs', C, MO_n5)\n",
    "MO_n5 = np.einsum('rC,ABrs->ABCs', C, MO_n5)\n",
    "MO_n5 = np.einsum('sD,ABCs->ABCD', C, MO_n5)\n",
    "n5_time = time.time() - n5_tstart\n",
    "print(\"...transformation complete in %.3f seconds.\" % n5_time)\n",
    "print(\"\\nN^5 %4.2f faster than N^8 algorithm!\" % (n8_time / n5_time))\n",
    "print(\"Allclose: %s\" % np.allclose(MO_n8, MO_n5))\n",
    "\n",
    "# Numpy einsum optimized transformation.\n",
    "print(\"\\nNow Numpy's optimized transformation...\")\n",
    "n8_tstart = time.time()\n",
    "MO_n8 = np.einsum('pI,qJ,pqrs,rK,sL->IJKL', C, C, I, C, C, optimize=True)\n",
    "n8_time_opt = time.time() - n8_tstart\n",
    "print(\"...optimized transformation complete in %.3f seconds.\" % (n8_time_opt))\n",
    "\n",
    "# Numpy GEMM N^5 transformation.\n",
    "# Try to figure this one out!\n",
    "print(\"\\n\\nStarting Numpy's N^5 transformation with dot...\")\n",
    "dgemm_tstart = time.time()\n",
    "MO = np.dot(C.T, I.reshape(size, -1))\n",
    "MO = np.dot(MO.reshape(-1, size), C)\n",
    "MO = MO.reshape(size, size, size, size).transpose(1, 0, 3, 2)\n",
    "\n",
    "MO = np.dot(C.T, MO.reshape(size, -1))\n",
    "MO = np.dot(MO.reshape(-1, size), C)\n",
    "MO = MO.reshape(size, size, size, size).transpose(1, 0, 3, 2)\n",
    "dgemm_time = time.time() - dgemm_tstart\n",
    "print(\"...transformation complete in %.3f seconds.\" % dgemm_time)\n",
    "print(\"\\nAllclose: %s\" % np.allclose(MO_n8, MO))\n",
    "print(\"N^5 %4.2f faster than N^8 algorithm!\" % (n8_time / dgemm_time))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "p4env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
